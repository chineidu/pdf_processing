config:
  api_config:
    title: PDF Processing API
    name: "${.title}"
    description: A robust API for processing PDF documents, extracting text, and performing OCR operations.
    version: v1.0.0
    status: healthy
    prefix: /api/v1
    auth_prefix: "${.prefix}/auth"
    middleware:
      cors:
        allow_origins:
          - "*"
        allow_credentials: true
        allow_methods:
          - "*"
        allow_headers:
          - "*"

  database_config:
    pool_size: 30
    max_overflow: 10 # Allow N extra connections
    pool_timeout: 20 # Wait N seconds for connection
    pool_recycle: 1800 # Recycle connections after 30 minutes
    pool_pre_ping: true # Test connections before use
    echo: false
    expire_on_commit: false # False for async implementations

  pdf_processing_config:
    max_num_pages: 80
    max_file_size_bytes: 36700160 # 35*1024*1024 # 35MB
    perform_ocr: true
    use_gpu: true

  endpoint_policies_config:
    ratelimits:
      {
        "guest":
          {
            "requests_per_minute": 10,
            "max_concurrent_requests": 1,
            "special_endpoints":
              {
                "/predict/ner": { "limit": 2, "period": 60 },
                "/predict/sentiment": { "limit": 5, "period": 60 },
              },
          },
        "free":
          {
            "requests_per_minute": 60,
            "max_concurrent_requests": 5,
            "special_endpoints":
              {
                "/predict/ner": { "limit": 5, "period": 60 },
                "/predict/sentiment": { "limit": 10, "period": 60 },
              },
          },
        "plus":
          {
            "requests_per_minute": 200,
            "max_concurrent_requests": 10,
            "special_endpoints":
              {
                "/predict/ner": { "limit": 20, "period": 60 },
                "/predict/sentiment": { "limit": 100, "period": 60 },
              },
          },
        "pro":
          {
            "requests_per_minute": 1000,
            "max_concurrent_requests": 50,
            "special_endpoints":
              {
                "/predict/ner": { "limit": 100, "period": 60 },
                "/predict/sentiment": { "limit": 500, "period": 60 },
              },
          },
      }
    credit_costs:
      "/predict/ner": 2.0
      "/predict/sentiment": 1.5
      "/detect-anomaly": 3.0
      "/cluster": 2.5
      "/recommend": 4.0

  queue_config:
    high_priority_ml: high_priority_ml # Queue for high priority machine learning tasks
    medium_priority_ml: medium_priority_ml # Queue for normal priority machine learning tasks
    low_priority_ml: low_priority_ml # Queue for low priority machine learning tasks
    cleanups: periodic # Queue for cleanup tasks
    notifications: user_notifications # Queue for user notifications

    # Priority mapping for queues
    priority_config:
      sizes:
        # lower is higher priority
        high_priority: 10
        medium_priority: 50
        low_priority: 200
      weights:
        # higher is higher priority
        high_priority: 9
        medium_priority: 6
        low_priority: 3

  celery_config:
    task_config:
      task_serializer: json
      result_serializer: json
      accept_content: ["json"]
      timezone: UTC
      enable_utc: true
    # Task routing and queues
    task_routes:
      src.celery_app.tasks.periodic.*:
        { queue: "${config.queue_config.cleanups}" }

    worker_config:
      worker_prefetch_multiplier: 1 # Busy workers are only assigned 1 task
      task_acks_late: true # Acknowledge tasks after completion
      # This config ONLY works with pool_type = 'prefork' (default)
      worker_max_tasks_per_child: 100 # Limit the number of tasks a worker can process before being replaced
      worker_max_memory_per_child: 2621440 # Max memory (in kilobytes) a worker can use before being replaced

    beat_config:
      beat_schedule:
        cleanup_old_records:
          task: src.celery_app.tasks.periodic.cleanup_old_records
          schedule: 3600 # 1 hour
      health_check:
        task: src.celery_app.tasks.periodic.health_check
        schedule: 600 # 10 minutes

    other_config:
      result_expires: 18000 # 5 hours
      task_compression: null # Disable task compression. Use `gzip` for compression
      result_compression: null
      # rpc:// as a transport-only result backend; results vanish on broker restart
      # result_backend: rpc://
      result_backend_always_retry: true
      result_persistent: true
      result_backend_max_retries: 3

  rabbitmq_config:
    max_retries: 3
    retry_delay: 1
    connection_timeout: 5
    heartbeat: 60
    prefetch_count: 1
    queue_names:
      task_queue: storage_queue
      result_queue: result_queue
    topic_names:
      storage_topic: minio_events
    queue_priority: high
    dlq_config:
      dlq_name: storage_events_dlq
      dlx_name: storage_events_dlx
      ttl: 30000 # 30 seconds
